---
title: "lecture 7"
author: "Anuhya Balineni"
output: html_document
date: "2025-04-20"
---

```setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(nnet)
library(ISLR2)
```

# 1. INTRODUCTION

\< Introduce the models being used\>

# 2. Data

\< Describe the data\>

```{r}
data = Default
str(data)
```

## 2.1 Visualizing the data

### 2.1.1 Distribution of Balance

\<what does this figure mean?\>

```{r balance distribution}
ggplot(data, aes(x =balance, fill=default)) +
  geom_histogram(bins = 30, alpha = 0.7, position = 'identity' ) +
  labs(title = "Distribution of balance by Default Status",
       x = "Balance",
       y = "Count?")
 
```

### 2.1.2 Distribution of Income

\<what does this figure mean?\>

```{r}
ggplot(data, aes(x = income, fill = default)) +
  geom_histogram(bins = 30, alpha = 0.7, position = 'identity') +
  labs(title = "distribution of Income by Default Status",
       x = "Income",
       y = "Count")
```

```{r}
ggplot(data, aes(x = income, fill = student)) +
  geom_histogram(bins = 30, alpha = 0.7, position = 'identity') +
  labs(title = "distribution of Income by Default Status",
       x = "Income",
       y = "Count")
```

### 2.1.3 Student Status by Default

```{r}
ggplot(data, aes(x = student, fill = default)) +
  geom_bar(position ='dodge') +
  labs(title = "distribution of Income by Student Status",
       x = "Student",
       y = "Count")
```

# 4. Logistic Regression

### 4.1 Fitting the Model

\<Describe Logistic Regression\>

```{r}
logit_model = glm(default ~ balance, data = data, family = binomial)
summary(logit_model)
```

```{r}
data$predicted_prob = predict(logit_model, type = "response")
head(data)
```

### 4.2 Evaluate Model Performance

\<talk about our model and evaluation metrics\>

```{r}
threshold = 0.5
data$predicted_default = ifelse(data$predicted_prob > threshold, "yes", "No")
conf_matrix = table(data$predicted_default, data$default)
conf_matrix
```

```{r}
accuracy = sum(diag(conf_matrix)) / sum(conf_matrix)
accuracy
```

# 5. Multiple Logistic Regression

## 5.1 Fitting the Model

Here, we will include an **interaction term** between 'income' and 'student' that allows the effect of 'income'

```{r}
logit_mult_model = glm(default ~ balance + income * student, data=data, 
family=binomial)
summary(logit_mult_model)
```

## 5.2 Evaluating the Model

\<talk about evaluation metrics / interpretation\>

```{r}
data$mult_predicted_prob = predict(logit_mult_model, type = "response")
data$mult_predicted_default = ifelse(data$mult_predicted_prob > threshold, "Yes", "No")
conf_matrix_mult = table(data$mult_predicted_default, data$default)
conf_matrix_mult
```

```{r}
accuracy_mult = sum(diag(conf_matrix_mult)) / sum(conf_matrix_mult)
accuracy_mult
```

# 6. Multinomial Logistic Regression

## 6.1 Load the Data

```{r}
data2 = Carseats
data2$SalesCategory = cut(data2$Sales, breaks = 3, labels = c("Low", "Medium", "High"))
```

```{r}
multi_model = multinom(SalesCategory ~ Price + Income +Advertising,
data = data2)
summary(multi_model)
```

## 6.2 Make Predictions

```{r}
data2$nomial_predicted_salesCat = predict(multi_model)
head(data2)
```

## 6.3 Evaluate Model

```{r}
conf_matrix_multi = table(data2$nomial_predicted_salesCat, data2$SalesCategory)
conf_matrix_multi
```

```{r}
accuracy_multi = sum(diag(conf_matrix_multi)) / sum(conf_matrix_multi) 
accuracy_multi
```

# Assignment Section

### Background

Diabetes is a chronic disease affecting millions of individuals worldwide. Early detection through predictive modelling can help guide prevention and treatment. in this assignment , you will use logistic regression to predict whether an individual has diabetes using basic health information.

We will use the Pima Indians Diabetes Dataset, a commonly used dataset in health informatics available from the UCI Machine Learning Repository and built into the mlbench R package.

## Simple Logistic Regression

```{r}
#install.packages("mlbench")
library(mlbench)
data("PimaIndiansDuabetes")
df = PimaIndiansDiabetes
```

### Data Exploration and Summary Figures
summary(df)
str(df)
table(df$diabetes)
Also,Visualization like:
library(ggplot2)

ggplot(df, aes(x = glucose, fill = diabetes)) +
  geom_histogram(bins = 30, alpha = 0.6, position = 'identity') +
  labs(title = "Glucose Distribution by Diabetes Status", x = "Glucose", y = "Count")

### Fit a Simple Logistic Regression Model (Train & Test Split)
set.seed(123)
train_index <- sample(1:nrow(df), 0.7 * nrow(df))
train <- df[train_index, ]
test <- df[-train_index, ]

simple_model <- glm(diabetes ~ glucose, data = train, family = binomial)
summary(simple_model)

\< Fit a logistic regression using glucose as a predictors of diabetes \>

### Interpret Coefficients & Apply the Model for Prediction on the test Data
test$predicted_prob <- predict(simple_model, newdata = test, type = "response")
test$predicted_class <- ifelse(test$predicted_prob > 0.5, "pos", "neg")
table(Predicted = test$predicted_class, Actual = test$diabetes)

## Multiple Logistic Regression
multi_model <- glm(diabetes ~ glucose + age + bmi + pregnant, data = train, family = binomial)
summary(multi_model)

test$multi_predicted_prob <- predict(multi_model, newdata = test, type = "response")
test$multi_predicted_class <- ifelse(test$multi_predicted_prob > 0.5, "pos", "neg")
table(Predicted = test$multi_predicted_class, Actual = test$diabetes)
Missing Accuracy Calculation for Both Models

You already did a confusion matrix for both the simple and multiple logistic models. Just add the accuracy after each one, like this:

After the simple logistic model:

conf_matrix_simple <- table(Predicted = test$predicted_class, Actual = test$diabetes)
accuracy_simple <- sum(diag(conf_matrix_simple)) / sum(conf_matrix_simple)
accuracy_simple

After the multiple logistic model:

conf_matrix_multi <- table(Predicted = test$multi_predicted_class, Actual = test$diabetes)
accuracy_multi <- sum(diag(conf_matrix_multi)) / sum(conf_matrix_multi)
accuracy_multi
Model Comparison and Discussion
1. Simple Logistic Regression

Model: Uses only glucose to predict diabetes.

Interpretation: Glucose is a significant predictor; as glucose increases, the probability of having diabetes increases.

Performance: Decent baseline performance, but limited by using only one predictor.

Pros: Easy to interpret, fast to train.

Cons: May miss important interactions or effects from other variables.


2. Multiple Logistic Regression

Model: Includes glucose, age, BMI, and number of pregnancies.

Interpretation: More comprehensive; allows for better modeling of the underlying relationships.

Performance: Typically better than simple logistic regression due to inclusion of more relevant predictors.

Pros: Balanced between interpretability and accuracy.

Cons: Slightly more complex, may still be limited if interactions or nonlinearities are present.


3. K-Nearest Neighbors (KNN)

Model: Classifies individuals based on the most similar cases (neighbors).

Performance: Can perform well with enough data and good normalization.

Pros: No assumptions about the data distribution, flexible.

Cons: Sensitive to choice of k and data scaling, hard to interpret, slower on large datasets. 

