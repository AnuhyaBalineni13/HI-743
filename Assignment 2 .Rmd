---
title: "Lecture 7"
author: "Anuhya Balineni"
output: html_document
---

{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Background
Diabetes is a chronic condition impacting millions worldwide. Early detection through predictive modeling enables better prevention and management. In this analysis, we use the Pima Indians Diabetes Dataset available in the mlbench package to build predictive models using logistic regression and k-nearest neighbors (KNN).

{r}
# Load required packages
# install.packages("mlbench")
# install.packages("caret")
# install.packages("class")
# install.packages("dplyr")

library(mlbench)
library(caret)
library(class)
library(dplyr)

# Load the dataset
data("PimaIndiansDiabetes")
df <- PimaIndiansDiabetes

# Quick overview of the data
glimpse(df)
summary(df)
  
## Simple Logistic Regression

### Train/Test Split and Model Fit

{r}
set.seed(123)
split_index <- createDataPartition(df$diabetes, p = 0.7, list = FALSE)
train_data <- df[split_index, ]
test_data <- df[-split_index, ]

# Fit simple logistic regression using glucose as predictor
simple_logit <- glm(diabetes ~ glucose, data = train_data, family = "binomial")
summary(simple_logit)

### Prediction and Evaluation

{r}
# Predict probabilities
simple_preds_prob <- predict(simple_logit, newdata = test_data, type = "response")

# Convert probabilities to classes
simple_preds_class <- factor(ifelse(simple_preds_prob > 0.5, "pos", "neg"), levels = c("neg", "pos"))

# Evaluate performance
confusionMatrix(simple_preds_class, test_data$diabetes)

## Multiple Logistic Regression

### Model Fit

{r}
# Fit multiple logistic regression using glucose, age, mass (BMI), and pregnant
multi_logit <- glm(diabetes ~ glucose + age + mass + pregnant, data = train_data, family = "binomial")
summary(multi_logit)

### Prediction and Evaluation

{r}
# Predict probabilities
multi_preds_prob <- predict(multi_logit, newdata = test_data, type = "response")

# Convert probabilities to classes
multi_preds_class <- factor(ifelse(multi_preds_prob > 0.5, "pos", "neg"), levels = c("neg", "pos"))

# Evaluate performance
confusionMatrix(multi_preds_class, test_data$diabetes)

## K-Nearest Neighbors (KNN)

### Data Preparation

{r}
# Normalize numeric predictors
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

df_norm <- df %>%
  mutate(across(c(glucose, age, mass, pregnant), normalize))

# Recreate train/test sets
train_norm <- df_norm[split_index, ]
test_norm <- df_norm[-split_index, ]

# Prepare features and labels
train_features <- train_norm %>% select(glucose, age, mass, pregnant)
test_features <- test_norm %>% select(glucose, age, mass, pregnant)

train_labels <- train_norm$diabetes
test_labels <- test_norm$diabetes

### Model Fitting and Prediction

{r}
# Fit KNN model with k = 5
set.seed(123)
knn_predictions <- knn(train = train_features, test = test_features, cl = train_labels, k = 5)

# Evaluate performance
confusionMatrix(knn_predictions, test_labels)

## Model Comparison and Discussion

Simple Logistic Regression:

- Used only glucose to predict diabetes.
- Provides a simple model but may miss important context from other variables.

Multiple Logistic Regression:

- Included glucose, age, BMI (mass), and pregnancy count as predictors.
- Generally achieved better predictive performance because it used more relevant health information.

K-Nearest Neighbors (KNN):

- A flexible, non-parametric method.
- Model performance depends on the choice of *k* and careful normalization of features.
- Less interpretable compared to logistic regression.

Conclusion:

- Multiple logistic regression is preferable for interpretability and good accuracy.
- KNN can be useful but needs careful tuning and preprocessing.






 

 

